{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "PwC_Scraper.ipynb의 사본",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3cScCSFDH4H"
      },
      "source": [
        "# Took reference of this link:\n",
        "\n",
        "https://github.com/robertlandlord/glassdoor_scraper"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xqJDxjYJSUw"
      },
      "source": [
        "#Selenium is an open-source tool that automates web browsers.\n",
        "!pip install selenium\n",
        "!apt-get -q update   #Used to handle installation and removal of softwares and libraries\n",
        "!apt install -yq chromium-chromedriver #ChromeDriver is a separate executable that Selenium WebDriver uses to control Chrome.\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
        "\n",
        "import sys\n",
        "sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')\n",
        "from selenium import webdriver \n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "import warnings\n",
        "import traceback\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "#WebDriver is a browser automation framework that works with open source APIs. \n",
        "#The framework operates by accepting commands, sending those commands to a browser, and interacting with applications.\n",
        "\n",
        "\n",
        "\n",
        "from selenium.common.exceptions import NoSuchElementException\n",
        "from selenium import *\n",
        "import xlwt\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.common.by import By \n",
        "from selenium.webdriver.support.ui import WebDriverWait \n",
        "from selenium.webdriver.support import expected_conditions as EC \n",
        "from selenium.common.exceptions import TimeoutException"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTXwOcrzDH4L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e08f860-2cb6-4fd2-9a05-e747d387d754"
      },
      "source": [
        "option = webdriver.ChromeOptions() #enables chrome option functions\n",
        "option.add_argument(\"—incognito\") \n",
        "option.add_argument('--headless')\n",
        "option.add_argument('--no-sandbox') # we don't need Chrome sandbox for this project\n",
        "option.add_argument('--disable-dev-shm-usage') # prevents Chrome throwing error\n",
        "prefs = {'profile.managed_default_content_settings.images':2, 'disk-cache-size': 4096} # disables images\n",
        "option.add_experimental_option(\"prefs\", prefs)\n",
        "\n",
        "url_link = \"https://www.glassdoor.com/Reviews/PwC-Reviews-E8450_P1.htm\" # The link to scrape!\n",
        "\n",
        "# library xlwt allows us to put data into an Excel spreadsheet!!\n",
        "company_info = xlwt.Workbook(encoding=\"utf-8\")\n",
        "name = input(\"What would you like to name the file?\") + \".csv\" # name = \"pwc.csv\"\n",
        "\n",
        "def scrape(base_url, workbook, name): #workbook == xlwt workbook object\n",
        "    worksheet = workbook.add_sheet(\"sheet1\", cell_overwrite_ok=True)\n",
        "    # write headers\n",
        "    worksheet.write(0, 1, \"Title\")\n",
        "    worksheet.write(0, 2, \"Date Written\")\n",
        "    worksheet.write(0, 3, \"Rating\")\n",
        "    worksheet.write(0, 4, \"Current/Former\")\n",
        "    worksheet.write(0, 5, \"Job Title\")\n",
        "    worksheet.write(0, 6, \"Location\")\n",
        "    worksheet.write(0, 7, \"Recommendation?\")\n",
        "    worksheet.write(0, 8, \"Outlook\")\n",
        "    worksheet.write(0, 9, \"Main Text\")\n",
        "    worksheet.write(0, 10, \"Pros\")\n",
        "    worksheet.write(0, 11, \"Cons\")\n",
        "    # worksheet.write(0, 12, \"Advice to management\")\n",
        "    # setting first url\n",
        "    url = base_url\n",
        "    chrome_path = 'chromedriver'\n",
        "    driver = webdriver.Chrome(chrome_path, chrome_options=option)\n",
        "    # logging in\n",
        "    driver.get(\"https://www.glassdoor.com/profile/login_input.htm?userOriginHook=HEADER_SIGNIN_LINK\")\n",
        "    # entering username/password\n",
        "    username = driver.find_element_by_name(\"username\")\n",
        "    username.send_keys(\"hyeongu.kim@utexas.edu\")\n",
        "    password = driver.find_element_by_name(\"password\")\n",
        "    password.send_keys(\"cheroke7\")\n",
        "    login_btn = driver.find_element_by_xpath(\"//*[@class='gd-ui-button minWidthBtn css-8i7bc2']\")\n",
        "    driver.execute_script(\"arguments[0].click();\", login_btn)\n",
        "    \n",
        "    # after login\n",
        "    try:\n",
        "        driver.get(url) # go to the first link\n",
        "        url_arr = url.split(\"_\") # [\"https://www.glassdoor.com/Reviews/PwC-Reviews-E8450\", \"_P1.htm\"]\n",
        "        basic = url_arr[0]+\"_P\" # \"https://www.glassdoor.com/Reviews/PwC-Reviews-E8450\" + \"_P\"\n",
        "        page_arr = list(url_arr[1]) #['P', '1', '.', 'h', 't', 'm']\n",
        "        page = \"\"\n",
        "        for char in page_arr:\n",
        "            if char.isdigit():\n",
        "                page += char # page is now = 1\n",
        "        # this is the current page we are on\n",
        "        page = int(page)\n",
        "        counter = 0\n",
        "        # print(str(url_arr))\n",
        "        while True:\n",
        "            # search the company\n",
        "            # ~~~~~~~~~~~~~~~~~~~TITLE OF REVIEW~~~~~~~~~~~~~~~~~~~~~~\n",
        "            titles = driver.find_elements_by_class_name(\"mb-xxsm.mt-0.css-5j5djr\") # find all the titles under this class (about 10 titles per page)\n",
        "            # print(\"titles: \" + str(titles))\n",
        "            titlearr = []\n",
        "            for title in titles:\n",
        "                counter += 1\n",
        "                titlearr.append(title.text)\n",
        "\n",
        "            # ~~~~~~~~~~~~~~~~~~~~TIMESTAMPS~~~~~~~~~~~~~~~~~~~~~~~\n",
        "            timestamps = driver.find_elements_by_class_name(\"authorJobTitle.middle.common__EiReviewDetailsStyle__newGrey\") # returns a list of timestamps like \"Sep 13, 2014 - Advisory Director\"\n",
        "            # print(\"timestamps: \" + str(timestamps))\n",
        "            datelist = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
        "            true_timestamps = []\n",
        "            job_titles = []\n",
        "            for datetime in timestamps:\n",
        "                true_timestamps.append(datetime.text.split(\"-\")[0])  #[\"Sep 13, 2014\", \"Advisory Director\"]\n",
        "                job_titles.append(datetime.text.split(\"-\")[1])\n",
        "\n",
        "\n",
        "\n",
        "            # ~~~~~~~~~~~~~~~~~~~RATINGS~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "            ratings = driver.find_elements_by_class_name(\"ratingNumber.mr-xsm\")\n",
        "            # print(\"ratings: \" + str(ratings))\n",
        "            ratingarr = []\n",
        "            for rating in ratings:  # NEED TO REMOVE THE FIRST ONE BECAUSE ITS GETTING OVERALL RATING\n",
        "                ratingarr.append(rating.text)\n",
        "            #if int(page) == 1:\n",
        "                #del ratingarr[0]\n",
        "\n",
        "            # ~~~~~~~~~~~~~~~~~~~~CURRENT-FORMER STATUS~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "            cfstatus = driver.find_elements_by_class_name(\"pt-xsm.pt-md-0.css-1qxtz39.eg4psks0\")\n",
        "            cfstatusarr = []\n",
        "            for status in cfstatus:\n",
        "              cfstatusarr.append(status.text.split(\" \")[0])\n",
        "\n",
        "            # ~~~~~~~~~~~~~~~~~~~~~~~~~FULL REVIEW~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "            triples = driver.find_elements_by_xpath('//*[@class=\"gdReview\"]') # a list of full reviews\n",
        "            # these lists eventually gonna be in each columns of the spreadsheet\n",
        "            recarr = []; outlookarr = []; othertextarr = []; prosarr = []; consarr = []; advarr = []; \n",
        "\n",
        "            #iterate through each full reviews\n",
        "            for triple in triples:\n",
        "                recommends = \"\"; outlook = \"\"; othertext = \"\"; pros = \"\"; cons = \"\"; advice = \"\";\n",
        "                textlist = triple.text.splitlines() # split by each line\n",
        "                # Scrape necessary info from each review:\n",
        "                for index in range(0, len(textlist) - 1):\n",
        "                    if \"Recommend\" in textlist[index]:\n",
        "                        recommends = textlist[index]\n",
        "                    elif \"Outlook\" in textlist[index]:\n",
        "                        outlook = textlist[index]\n",
        "                    elif \"Pros\" == textlist[index]:\n",
        "                        pros = textlist[index + 1]\n",
        "                        textlist[index + 1] = \"got\"\n",
        "                    elif \"Cons\" == textlist[index]:\n",
        "                        cons = textlist[index + 1]\n",
        "                        textlist[index + 1] = \"got\"\n",
        "                    elif \"Advice to Management\" in textlist[index]:\n",
        "                        advice = textlist[index + 1]\n",
        "                        textlist[index + 1] = \"got\"\n",
        "                    elif \"got\" == textlist[index]:\n",
        "                        continue\n",
        "                    else:\n",
        "                        othertext = textlist[index]\n",
        "                # Append\n",
        "                recarr.append(recommends); outlookarr.append(outlook); othertextarr.append(othertext); \n",
        "                prosarr.append(pros); consarr.append(cons); advarr.append(advice);\n",
        "\n",
        "            # Insert the data scraped into the Excel spreadsheet:\n",
        "            for index in range(0, len(titlearr)):\n",
        "                # print(\"INDEX IS: \", index)\n",
        "                # print(\"ROW: \", int(page)*10+index)\n",
        "                worksheet.write(int(page)*10+index, 1, titlearr[index])\n",
        "                worksheet.write(int(page)*10+index, 2, true_timestamps[index])\n",
        "                worksheet.write(int(page)*10+index, 3, ratingarr[index])\n",
        "                worksheet.write(int(page)*10+index, 4, cfstatusarr[index])\n",
        "                worksheet.write(int(page)*10+index, 5, job_titles[index])\n",
        "                # worksheet.write(int(page)*10+index, 6, locarr[index])\n",
        "                worksheet.write(int(page)*10+index, 7, recarr[index])\n",
        "                worksheet.write(int(page)*10+index, 8, outlookarr[index])\n",
        "                worksheet.write(int(page)*10+index, 9, othertextarr[index])\n",
        "                worksheet.write(int(page)*10+index, 10, prosarr[index])\n",
        "                worksheet.write(int(page)*10+index, 11, consarr[index])\n",
        "                # worksheet.write(int(page)*10+index, 12, advarr[index])\n",
        "            if page == 200:\n",
        "              print(\"Saving workbook...\")\n",
        "              workbook.save(name)\n",
        "              return\n",
        "            try:\n",
        "                found = driver.find_element_by_css_selector(\"#FooterPageNav > div > ul > li.page.current.last > span\")\n",
        "                print(found.text)\n",
        "                workbook.save(name)\n",
        "                break\n",
        "\n",
        "            # This will probably run every time.\n",
        "            except NoSuchElementException:\n",
        "                page += 1\n",
        "                driver.get(basic+str(page)+\".htm\")\n",
        "                print(\"clicked page\", str(page))\n",
        "\n",
        "\n",
        "    # except StaleElementReferenceException as e:\n",
        "    #   print(e)\n",
        "    #   print(traceback.format_exc())\n",
        "    #   workbook.save(name)\n",
        "    #   print(\"finished\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        print(e.args)\n",
        "        workbook.save(name)\n",
        "        print(\"finished\")\n",
        "\n",
        "\n",
        "                #(xlwt object)\n",
        "scrape(url_link, company_info, name)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What would you like to name the file?PwC_Reviews\n",
            "clicked page 2\n",
            "clicked page 3\n",
            "Message: stale element reference: element is not attached to the page document\n",
            "  (Session info: headless chrome=93.0.4577.63)\n",
            "\n",
            "('stale element reference: element is not attached to the page document\\n  (Session info: headless chrome=93.0.4577.63)', None, None)\n",
            "finished\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAOSLAYkPQGK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}